name: Core-2: Processing Engine

# Consolidated Processing Engine Workflow
# Combines: Project Self-Improver, Issue Auto-Responder, Project Audit,
#           PR Analyzer, and Hardened Analysis
# Part of Workflow Consolidation: 46 workflows â†’ 8 cores

"on":
  # Event triggers
  push:
    branches: [main, develop, feature/*, hotfix/*]
  pull_request:
    types: [opened, synchronize, reopened, closed, merged]
  issues:
    types: [opened, edited, reopened, closed, labeled, unlabeled]
  issue_comment:
    types: [created, edited, deleted]
  # Schedule triggers
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours for continuous improvement
    - cron: '0 */2 * * *'  # Every 2 hours for issue monitoring
    - cron: '0 */8 * * *'  # Every 8 hours for project audit
  # Manual dispatch
  workflow_dispatch:
    inputs:
      processing_mode:
        description: 'Processing Mode'
        required: true
        default: 'intelligent'
        type: choice
        options:
          - intelligent
          - project-analysis
          - issue-processing
          - pr-analysis
          - audit-processing
          - hardened-analysis
          - all
      improvement_mode:
        description: 'Improvement Mode (for project analysis)'
        required: false
        default: 'intelligent'
        type: choice
        options:
          - intelligent
          - aggressive
          - conservative
          - performance_focused
          - security_focused
          - documentation_focused
      response_mode:
        description: 'Response Mode (for issue processing)'
        required: false
        default: 'intelligent'
        type: choice
        options:
          - intelligent
          - aggressive
          - conservative
          - technical_focused
          - user_friendly
          - automated_fix
      audit_mode:
        description: 'Audit Mode (for audit processing)'
        required: false
        default: 'comprehensive'
        type: choice
        options:
          - comprehensive
          - security_focused
          - performance_focused
          - documentation_focused
          - compliance_focused
          - architecture_focused
      target_areas:
        description: 'Target Areas (comma-separated)'
        required: false
        default: 'all'
        type: string
      learning_depth:
        description: 'Learning Depth'
        required: false
        default: 'deep'
        type: choice
        options:
          - surface
          - medium
          - deep
          - comprehensive
      auto_apply:
        description: 'Auto-apply improvements'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  PROCESSING_MODE: ${{ github.event.inputs.processing_mode || 'intelligent' }}
  IMPROVEMENT_MODE: ${{ github.event.inputs.improvement_mode || 'intelligent' }}
  RESPONSE_MODE: ${{ github.event.inputs.response_mode || 'intelligent' }}
  AUDIT_MODE: ${{ github.event.inputs.audit_mode || 'comprehensive' }}
  TARGET_AREAS: ${{ github.event.inputs.target_areas || 'all' }}
  LEARNING_DEPTH: ${{ github.event.inputs.learning_depth || 'deep' }}
  AUTO_APPLY: ${{ github.event.inputs.auto_apply || 'false' }}

jobs:
  # Job 1: AI Project Analysis (via Orchestrator)
  ai-project-analysis:
    name: ðŸ§  AI Project Analysis
    uses: ./.github/workflows/00-zero-failure-ai-orchestrator.yml
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'project-analysis' || env.PROCESSING_MODE == 'all'
    with:
      task_type: pr_analysis
      system_message: |
        You are an expert project analyzer. Analyze the project and provide:
        1. Project structure assessment
        2. Code quality analysis
        3. Performance bottlenecks
        4. Security vulnerabilities
        5. Improvement recommendations
        6. Learning insights
      user_prompt: |
        Analyze this project for self-improvement:
        
        Repository: ${{ github.repository }}
        Branch: ${{ github.ref_name }}
        Commit: ${{ github.sha }}
        Improvement Mode: ${{ env.IMPROVEMENT_MODE }}
        Target Areas: ${{ env.TARGET_AREAS }}
        Learning Depth: ${{ env.LEARNING_DEPTH }}
        
        Provide comprehensive analysis with actionable improvement recommendations.
      max_tokens: 5000
      temperature: 0.7
      use_cache: true
    secrets: inherit
    continue-on-error: true

  # Job 2: Project Analysis & Learning
  project_analysis_learning:
    name: ðŸ” Project Analysis & Learning
    needs: [ai-project-analysis]
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'project-analysis' || env.PROCESSING_MODE == 'all'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install --prefer-binary PyYAML requests aiohttp
        pip install openai anthropic google-generativeai groq cohere || true
        pip install gitpython pygit2 || true
        pip install flake8 bandit pytest || true
    
    - name: ðŸ” Run Project Analysis
      id: project_analysis
      env:
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        GPT4_API_KEY: ${{ secrets.GPT4_API_KEY }}
        GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
        GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
        KIMI_API_KEY: ${{ secrets.KIMI_API_KEY }}
        QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GPTOSS_API_KEY: ${{ secrets.GPTOSS_API_KEY }}
        GROQAI_API_KEY: ${{ secrets.GROQAI_API_KEY }}
        CEREBRAS_API_KEY: ${{ secrets.CEREBRAS_API_KEY }}
        GEMINIAI_API_KEY: ${{ secrets.GEMINIAI_API_KEY }}
        COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
        NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        CODESTRAL_API_KEY: ${{ secrets.CODESTRAL_API_KEY }}
        GEMINI2_API_KEY: ${{ secrets.GEMINI2_API_KEY }}
        GROQ2_API_KEY: ${{ secrets.GROQ2_API_KEY }}
        CHUTES_API_KEY: ${{ secrets.CHUTES_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        set -o pipefail
        echo "ðŸ” Starting Project Analysis & Learning"
        echo "Mode: $IMPROVEMENT_MODE | Areas: $TARGET_AREAS | Depth: $LEARNING_DEPTH"
        
        # Run project analysis if script exists
        if [ -f ".github/scripts/ai_project_self_improver.py" ]; then
          python .github/scripts/ai_project_self_improver.py \
            --improvement-mode $IMPROVEMENT_MODE \
            --target-areas $TARGET_AREAS \
            --learning-depth $LEARNING_DEPTH \
            --auto-apply $AUTO_APPLY \
            --output project_analysis_results.json || true
        else
          # Fallback: Basic analysis
          python -c "
          import json
          from pathlib import Path
          
          analysis = {
              'timestamp': __import__('datetime').datetime.utcnow().isoformat(),
              'improvement_mode': '$IMPROVEMENT_MODE',
              'target_areas': '$TARGET_AREAS',
              'learning_depth': '$LEARNING_DEPTH',
              'total_files': len(list(Path('.').rglob('*.py'))),
              'status': 'success'
          }
          
          with open('project_analysis_results.json', 'w') as f:
              json.dump(analysis, f, indent=2)
          "
        fi
        
        echo "âœ… Project analysis completed"
    
    - name: ðŸ“¤ Upload Analysis Results
      uses: actions/upload-artifact@v4
      with:
        name: project-analysis-results
        path: project_analysis_results.json
        if-no-files-found: ignore
        retention-days: 30

  # Job 3: AI Issue Analysis (via Orchestrator)
  ai-issue-analysis:
    name: ðŸ§  AI Issue Analysis
    uses: ./.github/workflows/00-zero-failure-ai-orchestrator.yml
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'issue-processing' || env.PROCESSING_MODE == 'all' || github.event_name == 'issues' || github.event_name == 'issue_comment'
    with:
      task_type: pr_analysis
      system_message: |
        You are an expert issue analyzer. Analyze GitHub issues and provide:
        1. Issue categorization and priority
        2. Root cause analysis
        3. Suggested solutions
        4. Related issues or PRs
        5. Actionable recommendations
      user_prompt: |
        Analyze this GitHub Issue:
        
        Issue #${{ github.event.issue.number || 'N/A' }}
        Title: ${{ github.event.issue.title || 'N/A' }}
        Body: ${{ github.event.issue.body || 'N/A' }}
        Author: ${{ github.event.issue.user.login || 'N/A' }}
        Labels: ${{ toJson(github.event.issue.labels) || '[]' }}
        
        Repository: ${{ github.repository }}
        Response Mode: ${{ env.RESPONSE_MODE }}
        
        Provide comprehensive analysis with actionable recommendations.
      max_tokens: 3000
      temperature: 0.7
      use_cache: true
    secrets: inherit
    continue-on-error: true

  # Job 4: Issue Analysis & Categorization
  issue_analysis_categorization:
    name: ðŸ” Issue Analysis & Categorization
    needs: [ai-issue-analysis]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'issue-processing' || env.PROCESSING_MODE == 'all' || github.event_name == 'issues' || github.event_name == 'issue_comment'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install --prefer-binary PyYAML requests aiohttp
        pip install openai anthropic google-generativeai groq cohere || true
        pip install gitpython pygit2 || true
        pip install langdetect textblob || true
    
    - name: ðŸ” Run Issue Analysis
      id: issue_analysis
      env:
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        GPT4_API_KEY: ${{ secrets.GPT4_API_KEY }}
        GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
        GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
        KIMI_API_KEY: ${{ secrets.KIMI_API_KEY }}
        QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GPTOSS_API_KEY: ${{ secrets.GPTOSS_API_KEY }}
        GROQAI_API_KEY: ${{ secrets.GROQAI_API_KEY }}
        CEREBRAS_API_KEY: ${{ secrets.CEREBRAS_API_KEY }}
        GEMINIAI_API_KEY: ${{ secrets.GEMINIAI_API_KEY }}
        COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
        NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        CODESTRAL_API_KEY: ${{ secrets.CODESTRAL_API_KEY }}
        GEMINI2_API_KEY: ${{ secrets.GEMINI2_API_KEY }}
        GROQ2_API_KEY: ${{ secrets.GROQ2_API_KEY }}
        CHUTES_API_KEY: ${{ secrets.CHUTES_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -e
        set -o pipefail
        echo "ðŸ” Starting Issue Analysis & Categorization"
        echo "Response Mode: $RESPONSE_MODE"
        
        # Run issue analysis if script exists
        if [ -f ".github/scripts/ai_issue_responder.py" ]; then
          python .github/scripts/ai_issue_responder.py \
            --response-mode $RESPONSE_MODE \
            --output issue_analysis_results.json || true
        else
          # Fallback: Basic issue analysis
          python -c "
          import json
          
          analysis = {
              'timestamp': __import__('datetime').datetime.utcnow().isoformat(),
              'response_mode': '$RESPONSE_MODE',
              'issue_number': '${{ github.event.issue.number || \"N/A\" }}',
              'status': 'success'
          }
          
          with open('issue_analysis_results.json', 'w') as f:
              json.dump(analysis, f, indent=2)
          "
        fi
        
        echo "âœ… Issue analysis completed"
    
    - name: ðŸ“¤ Upload Issue Analysis Results
      uses: actions/upload-artifact@v4
      with:
        name: issue-analysis-results
        path: issue_analysis_results.json
        if-no-files-found: ignore
        retention-days: 30

  # Job 5: PR Analysis Processing
  pr_analysis_processing:
    name: ðŸ” PR Analysis Processing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'pr-analysis' || env.PROCESSING_MODE == 'all' || github.event_name == 'pull_request'
    permissions:
      pull-requests: write
      contents: read
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiohttp openai anthropic google-generativeai groq cohere mistralai || true
        pip install multidict yarl attrs aiosignal frozenlist python-dotenv || true
    
    - name: ðŸ¤– Run PR Analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CEREBRAS_API_KEY: ${{ secrets.CEREBRAS_API_KEY }}
        CHUTES_API_KEY: ${{ secrets.CHUTES_API_KEY }}
        CODESTRAL_API_KEY: ${{ secrets.CODESTRAL_API_KEY }}
        COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        GEMINI2_API_KEY: ${{ secrets.GEMINI2_API_KEY }}
        GEMINIAI_API_KEY: ${{ secrets.GEMINIAI_API_KEY }}
        GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
        GPTOSS_API_KEY: ${{ secrets.GPTOSS_API_KEY }}
        GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
        GROQ2_API_KEY: ${{ secrets.GROQ2_API_KEY }}
        GROQAI_API_KEY: ${{ secrets.GROQAI_API_KEY }}
        KIMI_API_KEY: ${{ secrets.KIMI_API_KEY }}
        NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
      run: |
        set -e
        set -o pipefail
        echo "ðŸ¤– Running PR Analysis..."
        mkdir -p artifacts final_results
        
        # Run PR analysis if script exists
        if [ -f ".github/scripts/comprehensive_pr_analyzer_bulletproof.py" ]; then
          python .github/scripts/comprehensive_pr_analyzer_bulletproof.py || true
          
          # Validate bulletproof real AI was used
          if [ -f "artifacts/auto_pr_analysis.json" ] && grep -q '"bulletproof_validated": true' artifacts/auto_pr_analysis.json; then
            echo "âœ… BULLETPROOF REAL AI VERIFIED!"
            cp artifacts/auto_pr_analysis.json comprehensive_pr_analysis.json || true
          else
            echo "âš ï¸ Bulletproof validation not found, continuing..."
          fi
        else
          # Fallback: Basic PR analysis
          python -c "
          import json
          
          analysis = {
              'timestamp': __import__('datetime').datetime.utcnow().isoformat(),
              'pr_number': '${{ github.event.pull_request.number || \"N/A\" }}',
              'status': 'success'
          }
          
          with open('comprehensive_pr_analysis.json', 'w') as f:
              json.dump(analysis, f, indent=2)
          "
        fi
        
        echo "âœ… PR analysis completed"
    
    - name: ðŸ“¤ Upload PR Analysis Results
      uses: actions/upload-artifact@v4
      with:
        name: pr-analysis-results
        path: |
          comprehensive_pr_analysis.json
          final_results/comprehensive_pr_analysis.json
          artifacts/auto_pr_analysis.json
        if-no-files-found: ignore
        retention-days: 30

  # Job 6: Audit Processing
  audit_processing:
    name: ðŸ” Audit Processing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'audit-processing' || env.PROCESSING_MODE == 'all'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install --prefer-binary PyYAML requests aiohttp
        pip install openai anthropic google-generativeai groq cohere || true
        pip install gitpython pygit2 || true
        pip install flake8 bandit pytest || true
        pip install sphinx mkdocs jupyter || true
    
    - name: ðŸ” Run Comprehensive Project Audit
      id: project_audit
      env:
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        GPT4_API_KEY: ${{ secrets.GPT4_API_KEY }}
        GLM_API_KEY: ${{ secrets.GLM_API_KEY }}
        GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
        KIMI_API_KEY: ${{ secrets.KIMI_API_KEY }}
        QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GPTOSS_API_KEY: ${{ secrets.GPTOSS_API_KEY }}
        GROQAI_API_KEY: ${{ secrets.GROQAI_API_KEY }}
        CEREBRAS_API_KEY: ${{ secrets.CEREBRAS_API_KEY }}
        GEMINIAI_API_KEY: ${{ secrets.GEMINIAI_API_KEY }}
        COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
        NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        CODESTRAL_API_KEY: ${{ secrets.CODESTRAL_API_KEY }}
        GEMINI2_API_KEY: ${{ secrets.GEMINI2_API_KEY }}
        GROQ2_API_KEY: ${{ secrets.GROQ2_API_KEY }}
        CHUTES_API_KEY: ${{ secrets.CHUTES_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        AUDIT_MODE: ${{ env.AUDIT_MODE }}
      run: |
        set -e
        set -o pipefail
        echo "ðŸ” Starting Comprehensive Project Audit"
        echo "Audit Mode: $AUDIT_MODE"
        
        # Run audit if script exists
        if [ -f ".github/scripts/ai_project_structure_auditor.py" ]; then
          python .github/scripts/ai_project_structure_auditor.py \
            --audit-mode $AUDIT_MODE \
            --output audit_processing_results.json || true
        else
          # Fallback: Basic audit
          python -c "
          import json
          from pathlib import Path
          
          audit = {
              'timestamp': __import__('datetime').datetime.utcnow().isoformat(),
              'audit_mode': '$AUDIT_MODE',
              'total_files': len(list(Path('.').rglob('*.py'))),
              'status': 'success'
          }
          
          with open('audit_processing_results.json', 'w') as f:
              json.dump(audit, f, indent=2)
          "
        fi
        
        echo "âœ… Audit processing completed"
    
    - name: ðŸ“¤ Upload Audit Results
      uses: actions/upload-artifact@v4
      with:
        name: audit-processing-results
        path: audit_processing_results.json
        if-no-files-found: ignore
        retention-days: 30

  # Job 7: Hardened Analysis
  hardened_analysis:
    name: ðŸ›¡ï¸ Hardened Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: env.PROCESSING_MODE == 'intelligent' || env.PROCESSING_MODE == 'hardened-analysis' || env.PROCESSING_MODE == 'all' || github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
      actions: read
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install detect-secrets || true
    
    - name: ðŸ” Determine Changed Files
      id: changes
      run: |
        echo "files=$(git diff --name-only origin/${{ github.base_ref }}...HEAD 2>/dev/null | grep -E '\\.py$' | tr '\n' ' ' || echo '')" >> $GITHUB_OUTPUT
    
    - name: âœ… Pre-verify Syntax and AST
      id: deterministic
      run: |
        set -e
        mkdir -p artifacts
        SYNTAX_OK=true
        AST_OK=true
        
        for f in ${{ steps.changes.outputs.files }}; do
          if [ -f "$f" ]; then
            echo "Validating $f"
            # Syntax check
            python -m py_compile "$f" 2>/dev/null || SYNTAX_OK=false
            # AST check
            python -c "import ast; ast.parse(open('$f', 'r', encoding='utf-8').read())" 2>/dev/null || AST_OK=false
          fi
        done
        
        echo "syntax_ok=$SYNTAX_OK" >> $GITHUB_OUTPUT
        echo "ast_ok=$AST_OK" >> $GITHUB_OUTPUT
        
        if [ "$SYNTAX_OK" = "true" ]; then
          echo "SYNTAX_CONFIRMED_OK=true" >> $GITHUB_ENV
        fi
    
    - name: ðŸ¤– Run AI Analyzer (with guardrails)
      env:
        SYNTAX_CONFIRMED_OK: ${{ env.SYNTAX_CONFIRMED_OK }}
      run: |
        echo "ðŸ¤– Running AI Analyzer with guardrails..."
        if [ -f ".github/scripts/bulletproof_ai_pr_analyzer.py" ]; then
          python .github/scripts/bulletproof_ai_pr_analyzer.py || echo "âš ï¸ Analyzer completed with warnings"
        else
          echo "â„¹ï¸ Analyzer script not found, skipping..."
        fi
    
    - name: ðŸ“¤ Upload Validation Receipt
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: validation-receipt
        path: artifacts/validation_receipt.json
        if-no-files-found: ignore
        retention-days: 30

  # Job 8: Processing Summary
  processing_summary:
    name: ðŸ“Š Processing Summary
    runs-on: ubuntu-latest
    needs: [project_analysis_learning, issue_analysis_categorization, pr_analysis_processing, audit_processing, hardened_analysis]
    if: always()
    
    steps:
    - name: ðŸ“¥ Download All Artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
        pattern: '*'
        merge-multiple: true
    
    - name: ðŸ“Š Generate Summary
      run: |
        echo "ðŸ“Š Generating Processing Engine Summary"
        
        cat > processing_summary.md << EOF
        # ðŸ“Š Core-2: Processing Engine Summary
        
        ## ðŸ“ˆ **PROCESSING OVERVIEW**
        - **Processing Mode**: ${{ env.PROCESSING_MODE }}
        - **Timestamp**: $(date)
        - **Repository**: ${{ github.repository }}
        - **Branch**: ${{ github.ref_name }}
        - **Commit**: ${{ github.sha }}
        
        ## âœ… **JOB STATUS**
        - **Project Analysis**: ${{ needs.project_analysis_learning.result }}
        - **Issue Analysis**: ${{ needs.issue_analysis_categorization.result }}
        - **PR Analysis**: ${{ needs.pr_analysis_processing.result }}
        - **Audit Processing**: ${{ needs.audit_processing.result }}
        - **Hardened Analysis**: ${{ needs.hardened_analysis.result }}
        
        ## ðŸ“¦ **ARTIFACTS GENERATED**
        - Project analysis results
        - Issue analysis results
        - PR analysis results
        - Audit processing results
        - Hardened analysis validation
        
        ## ðŸŽ¯ **NEXT STEPS**
        1. Review all processing results
        2. Apply recommended improvements
        3. Address identified issues
        4. Schedule next processing cycle
        
        ---
        *Generated by Core-2: Processing Engine - Consolidated from 5 workflows*
        EOF
        
        echo "âœ… Processing summary generated"
    
    - name: ðŸ“¤ Upload Summary
      uses: actions/upload-artifact@v4
      with:
        name: processing-summary
        path: processing_summary.md
        retention-days: 90
    
    - name: ðŸ“ Create Summary Comment
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('processing_summary.md', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

