# KEDA ScaledObject Configuration for AMAS
# Enables intelligent autoscaling based on metrics, queue depth, and SLO performance

apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: amas-orchestrator-scaler
  namespace: amas-prod
  labels:
    app: amas-orchestrator
    scaler-type: multi-metric
spec:
  scaleTargetRef:
    name: amas-orchestrator
    kind: Rollout
    apiVersion: argoproj.io/v1alpha1
  
  # Scaling behavior
  pollingInterval: 15    # Check metrics every 15 seconds
  cooldownPeriod: 180    # Wait 3 minutes before scaling down
  idleReplicaCount: 2    # Minimum replicas when no load
  minReplicaCount: 2     # Absolute minimum
  maxReplicaCount: 50    # Maximum for safety
  
  # Advanced scaling behavior
  advanced:
    restoreToOriginalReplicaCount: true
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300  # 5 minute stabilization
          policies:
          - type: Percent
            value: 10      # Scale down max 10% per period
            periodSeconds: 60
          - type: Pods
            value: 2       # Scale down max 2 pods per period
            periodSeconds: 60
          selectPolicy: Min
        scaleUp:
          stabilizationWindowSeconds: 60   # 1 minute stabilization
          policies:
          - type: Percent
            value: 100     # Scale up max 100% per period
            periodSeconds: 60
          - type: Pods
            value: 5       # Scale up max 5 pods per period
            periodSeconds: 60
          selectPolicy: Max
  
  # Scaling triggers
  triggers:
  # Primary trigger: HTTP request rate
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      query: |
        rate(amas_agent_requests_total[2m])
      threshold: '15'  # Scale up when >15 RPS
      activationThreshold: '5'  # Start scaling at 5 RPS
    
  # Secondary trigger: Agent queue depth
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: agent_queue_depth
      query: |
        amas_queue_depth_current
      threshold: '25'  # Scale up when queue >25 items
      activationThreshold: '10'
    
  # Performance trigger: High latency indicates need for more capacity
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: latency_based_scaling
      query: |
        (
          histogram_quantile(0.95, 
            rate(amas_agent_duration_seconds_bucket[2m])
          ) > 1.0
        ) * 10  # Convert boolean to scaling signal
      threshold: '5'   # Scale when p95 latency > 1.0s
      activationThreshold: '1'
    
  # Resource utilization trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: cpu_memory_pressure
      query: |
        (
          avg(rate(container_cpu_usage_seconds_total{pod=~"amas-orchestrator-.*"}[2m])) * 100 > 70
        ) or (
          avg(container_memory_working_set_bytes{pod=~"amas-orchestrator-.*"} / 
              container_spec_memory_limit_bytes{pod=~"amas-orchestrator-.*"}) * 100 > 80
        )
      threshold: '0.5'  # Scale when either CPU >70% or Memory >80%
      activationThreshold: '0.1'

---
# Worker scaling configuration
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: amas-worker-scaler
  namespace: amas-prod
  labels:
    app: amas-worker
    scaler-type: queue-based
spec:
  scaleTargetRef:
    name: amas-worker
    
  minReplicaCount: 1
  maxReplicaCount: 20
  pollingInterval: 10
  cooldownPeriod: 120
  
  triggers:
  # Redis queue depth trigger
  - type: redis
    metadata:
      address: redis.amas-prod.svc.cluster.local:6379
      listName: agent_jobs
      listLength: '8'     # Scale up when >8 jobs queued
      activationListLength: '3'  # Start scaling at 3 jobs
      databaseIndex: '0'
      
  # Redis stream trigger for real-time events
  - type: redis-streams
    metadata:
      address: redis.amas-prod.svc.cluster.local:6379
      stream: agent_events
      consumerGroup: amas-workers
      pendingEntriesCount: '5'  # Scale when >5 pending events
      
  # CPU utilization for workers
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: worker_cpu_utilization
      query: |
        avg(rate(container_cpu_usage_seconds_total{pod=~"amas-worker-.*"}[2m])) * 100
      threshold: '65'  # Scale when average CPU >65%

---
# Analysis and Research Agent Scaling
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: amas-research-agent-scaler
  namespace: amas-prod
  labels:
    app: amas-research-agent
    scaler-type: demand-based
spec:
  scaleTargetRef:
    name: amas-research-agent
    
  minReplicaCount: 1
  maxReplicaCount: 10
  pollingInterval: 20
  cooldownPeriod: 300  # Longer cooldown for research agents
  
  triggers:
  # Research request queue
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: research_requests_queued
      query: |
        amas_queue_depth_current{queue_name="research_requests"}
      threshold: '5'
      
  # Token usage rate (scale up when token consumption is high)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: research_token_usage_rate
      query: |
        rate(amas_tokens_used_total{agent_id="research_agent_v1"}[5m])
      threshold: '1000'  # Scale when >1000 tokens/second

---
# Horizontal Pod Autoscaler for backup scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: amas-orchestrator-hpa-backup
  namespace: amas-prod
  labels:
    app: amas-orchestrator
    scaler-type: backup
spec:
  scaleTargetRef:
    apiVersion: argoproj.io/v1alpha1
    kind: Rollout
    name: amas-orchestrator
  
  minReplicas: 2
  maxReplicas: 30
  
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
        
  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
        
  # Custom metric: requests per second
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
        selector:
          matchLabels:
            app: amas-orchestrator
      target:
        type: AverageValue
        averageValue: '20'  # 20 RPS per pod
  
  # Scaling behavior (fallback if KEDA fails)
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 15
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 3
        periodSeconds: 60
      selectPolicy: Max

---
# Vertical Pod Autoscaler for right-sizing
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: amas-orchestrator-vpa
  namespace: amas-prod
spec:
  targetRef:
    apiVersion: argoproj.io/v1alpha1
    kind: Rollout
    name: amas-orchestrator
  
  updatePolicy:
    updateMode: "Auto"  # Automatically apply recommendations
    
  resourcePolicy:
    containerPolicies:
    - containerName: orchestrator
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      maxAllowed:
        cpu: "2000m" 
        memory: "2Gi"
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits

---
# Pod Disruption Budget for availability during scaling
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: amas-orchestrator-pdb
  namespace: amas-prod
spec:
  selector:
    matchLabels:
      app: amas-orchestrator
  maxUnavailable: 25%  # Allow up to 25% of pods to be unavailable

---
# Network Policy for security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: amas-orchestrator-netpol
  namespace: amas-prod
spec:
  podSelector:
    matchLabels:
      app: amas-orchestrator
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # Allow ingress from nginx controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000
      
  # Allow Prometheus scraping
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8080
  
  egress:
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
      
  # Allow HTTPS to external APIs
  - to: []
    ports:
    - protocol: TCP
      port: 443
      
  # Allow Redis access
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
      
  # Allow database access
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
